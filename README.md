# SatelliteBuildingArea
Определение площади застройки по спутниковому снимку.

## Эксперимент 1: сегментация зданий на спутниковых снимках (DeepLabV3)

Ноутбук: 1. Buildings_ResNet.ipynb

**Цель:** обучить модель бинарной сегментации зданий (маска 0/1) по спутниковым TIFF.

### Данные и разбиение
- Структура датасета: `dataset/images/*.tif` и `dataset/gt/*.tif` (маски с теми же именами файлов).
- Разбиение: train/val = 85/15 (фиксированный `seed=42`).

### Пайплайн данных
- Чтение больших TIFF “окнами” (`rasterio`), генерация случайных патчей на лету.
- Размер патча: **256×256**.
- Train: **scale jitter 0.5–1.5** + аугментации (**flip**, **rot90**).
- Val: без scale jitter и без аугментаций.
- Нормализация изображения в диапазон **[0, 1]**; маска бинаризуется **0/255 → 0/1**.

### Модель
- `DeepLabV3-ResNet50` (torchvision), `num_classes=1` (логиты для бинарной сегментации).

### Функция потерь
- `BCEWithLogitsLoss(pos_weight=5.0)` + `DiceLoss` (компенсация дисбаланса классов и оптимизация перекрытия).

### Обучение
- Оптимизатор: `AdamW(lr=lr 2e-4, weight_decay=1e-4)` (после выхода на плато дообучение на пониженном LR 5e-5).
- Mixed precision (AMP) + gradient accumulation: `batch=8`, `accum=4` (эффективный batch = 32).
- Gradient clipping: `max_grad_norm=1.0`.
- Resume и сохранение чекпойнтов: `best_model.pt` (лучший по IoU) и `last_model.pt`.

### Метрики
- Precision, Recall, IoU, F1 (по пикселям) после пороговой бинаризации вероятностей.

### Результаты
- На валидации в процессе дообучения: **IoU ~ 0.72–0.74**, **F1 ~ 0.83–0.85**.
- Лучший сохранённый чекпойнт: `best_iou ≈ 0.749` (epoch 47).

### Подбор порога для инференса
- Полный скан по порогам на val: лучший IoU при **thr = 0.78**  
  **IoU = 0.7413**, **P = 0.834**, **R = 0.870**, **F1 = 0.8514**.
- Проверка площади на val при `thr=0.78`: **Pred/GT ≈ 1.043** (≈ +4.3% площади относительно GT).
- **Лучший порог по площади (минимизация |Pred−GT|):** среди протестированных значений наиболее близкое совпадение площади даёт **thr = 0.80**  
  (Δ_area ≈ **+382,957 px**, **Pred/GT ≈ 1.026** на скане; площадь ближе к GT, но IoU практически не меняется: **IoU ≈ 0.7412**).

### Визуальная проверка
- Добавлен интерактивный просмотр (ipywidgets): выбор `best/last`, настройка порога, визуализация `GT / Pred / Overlay` + сравнение площадей.

## Эксперимент 2: U-Net с энкодером ResNet50 (transfer из DeepLabV3)

Ноутбук: 2. Buildings_UNet.ipynb

**Цель:** попробовать альтернативную архитектуру (U-Net) для бинарной сегментации зданий и улучшить качество относительно DeepLabV3 за счёт переносa признаков энкодера.

### Данные и пайплайн
- Используется тот же датасет `images/*.tif` + `gt/*.tif`, разбиение train/val = 85/15 (`seed=42`).
- Патчи **256×256**, чтение TIFF “окнами” (`rasterio`).
- Train: `scale jitter 0.5–1.5` + аугментации (flip, rot90).  
- Val: без jitter/аугментаций.
- Нормализация входа в **[0, 1]**, маска **0/255 → 0/1**.

### Модель
- **U-Net** с энкодером **ResNet50** (torchvision).  
- Выход: 1 канал логитов (`{"out": logits}`), чтобы совпасть с существующим training loop.

### Transfer learning (инициализация энкодера)
- Энкодер U-Net инициализируется весами из лучшего чекпойнта **DeepLabV3**:
  - из state_dict берутся ключи с префиксом `backbone.*` и загружаются в `model.encoder`.
- Декодер и голова U-Net — новые (обучаются с нуля).

### Функция потерь и метрики
- Loss: `BCEWithLogitsLoss(pos_weight=5.0)` + `DiceLoss`.
- Метрики: Precision / Recall / IoU / F1 (по пикселям), порог по умолчанию **thr=0.80**.

### Обучение
- `AdamW(weight_decay=1e-4)` с разными LR:
  - encoder: **1e-5** (очень маленький, чтобы не “сломать” признаки), после достижения плато снижаем до **3e-6**,
  - decoder/head: **1e-4** (выше, чтобы быстрее обучить декодер), после достижения плато снижаем до **3e-5**.
- AMP + gradient accumulation: `batch=8`, `accum=4` (эффективный batch=32).
- Gradient clipping: `max_grad_norm=1.0`.
- Resume из `/content/drive/MyDrive/Models/last_unet.pt`, сохранение `best_unet.pt` и `last_unet.pt`.

### Результаты (валидация)
- Лучший сохранённый чекпойнт: `best_iou ≈ 0.755` (epoch 50).

### Подбор порога для инференса
- Полный скан порогов на val показал лучший IoU при **thr = 0.80**:  
  **IoU = 0.7483**, **P = 0.842**, **R = 0.870**, **F1 = 0.8560**.
- **Лучший порог по площади (минимизация |Pred−GT| среди протестированных):** также **thr = 0.80**  
  (на скане Δ_area = **+467,899 px**, **Pred/GT = 1.033**).
- Итоговая проверка площади по всему val при `thr=0.80`: **Pred/GT ≈ 1.057** (≈ +5.7% площади относительно GT).

### Визуальная проверка
- Добавлен интерактивный просмотр (ipywidgets): выбор `best/last`, настройка порога, визуализация `GT / Pred / Overlay` + сравнение площадей.

## Эксперимент 3: Новая аугментация: имитация окклюзий “кронами деревьев”

Ноутбук: 3. Buildings_UNet_(trees).ipynb

- В `CropConfig` добавлены параметры `tree_*`, и в `_augment()` включена опция **tree occlusion**.
- Реализована функция `add_tree_blobs()`: поверх изображения накладываются полупрозрачные “пятна” (зелёные/иногда чёрные), сформированные из набора размытых кругов + шумовой текстуры.
- Центры пятен с вероятностью `tree_on_mask_prob` выбираются **по пикселям здания** (то есть окклюзия чаще закрывает здания, а не фон).

Параметры для train:
- `tree_p=0.30` (30% патчей с окклюзией)
- 2–6 пятен на патч (`tree_blobs`)
- радиусы 3–15 px (`tree_r_px`)
- непрозрачность 0.70–1.00 (`tree_alpha`)
- небольшое размытие `k=3..7` (`tree_blur_k`)
- `tree_black_prob=0.20`

Валидация: `tree_p=0.0` (без окклюзий).

### Порог для метрик и инференса
- Базовый порог в `metrics_from_logits()` и в визуализации изменён на **thr=0.86** (в прошлом ноутбуке был 0.80).
- Скан порогов смещён в “верхнюю” область: теперь тестируются **0.70–0.94** с шагами (0.70, 0.75, 0.80, 0.82, 0.84, 0.86, 0.88, 0.90, 0.92, 0.94).

### Результаты подбора порога (best U-Net)
- Лучший IoU на full val при **thr = 0.84**:  
  **IoU = 0.7581**, **P = 0.854**, **R = 0.871**, **F1 = 0.8624**.
- Лучший порог по площади (минимизация |Pred−GT| среди протестированных) — **thr = 0.86**:  
  на скане Δ_area = **−13,048 px**, **Pred/GT = 0.999** (практически идеальное совпадение площади).
- Полная проверка площади при `thr=0.84`: **Pred/GT ≈ 1.035** (≈ +3.5% площади относительно GT).

### Чекпойнты и прогресс обучения
- Продолжение обучения шло из `last_unet.pt` 
- После эпохи epoch=72 (best_iou ≈ 0.7612).

## Эксперимент 4: Регрессия GSD (оценка масштаба/разрешения) CNN моделью по патчам со спутниковых снимков

Ноутбук: 4. Buildings_UNet_(Scale) (mask only).ipynb

**Цель:** обучить модель, которая по изображению патча предсказывает **GSD (meters per pixel)**, чтобы далее корректно пересчитывать площадь зданий в физические единицы при разном масштабе/зуме/ресэмплинге.

---

### Данные и разбиение
- Используется тот же датасет сцен:
  - изображения: `dataset/images/*.tif`
  - маски зданий: `dataset/gt/*.tif` (те же имена файлов)
- Разбиение: train/val = **85/15** (`seed=42`).
- **Фильтрация пустых патчей:** при формировании выборки **отбрасываются** окна, где в маске нет зданий (`mask window sum == 0`).  
  Это важно, чтобы модель училась по информативным структурам (здания/контуры), а не по “пустому фону”.

---

### Пайплайн данных (GSD-датасет)
- Чтение больших TIFF “окнами” через `rasterio`, генерация патчей **на лету**.
- Размер выходного патча: **256×256**.
- Вводится “виртуальный zoom” **z**, который имитирует разные разрешения:
  - `z ~ U_log(0.25, 1.5)` (log-uniform sampling)
  - исходный размер окна: `src_sz ≈ patch_size / z`, затем ресайз обратно в 256×256
  - целевой GSD патча: `GSD = GSD0 / z`, где `GSD0 = 0.30 м/пикс` (базовый GSD тренировочного датасета)
- Таргет для регрессии: **log(GSD)** (стабильнее, мультипликативная шкала).
- Нормализация входа: изображение приводится к **[0, 1]**, поддерживаются `uint8/uint16/float` TIFF.

---

### Аугментации (anti-cheat, для устойчивости к “реальному миру”)
В train включены аугментации, которые имитируют разные источники данных и деградации:

- Геометрия: `flip`, `rot90`.
- Photometric jitter (с вероятностью `color_p=0.85`):
  - gamma: `0.85–1.15`
  - brightness/contrast/saturation: `±0.12`
- Blur: `p=0.20`, `sigma 0.2–1.0`
- Sharpen (unsharp mask): `p=0.12`, `amount 0.2–0.6`, `sigma 0.6–1.4`
- Noise: `p=0.30`, `std 0.0–0.02`
- JPEG-like деградация: `p=0.30`, квантование уровней (`q 32–128`) + downscale `0.6–1.0`
- “Shade” (мягкое затенение): `p=0.20`, сила `0.02–0.10`

Валидация:
- **scale jitter включён**, но **augment выключен** (чистая оценка, кроме изменения масштаба).

---

### Модель
- **ResNet50 encoder → регрессор GSD**
- Архитектура:
  - backbone: `resnet50(weights=None)` (fc удалён)
  - GAP (global average pooling) → MLP head:
    - `2048 → 256 → 1`
    - `Dropout=0.2`
- Выход: `{"t": pred_log_gsd}`, где `t = log(GSD)`.

---

### Transfer learning (инициализация энкодера из U-Net)
- Энкодер ResNet50 **инициализируется** из чекпойнта U-Net (из экспериментов 2–3):
  - из state_dict берутся ключи `encoder.*` (fallback: `model.encoder.*`)
  - загружаются в `gsd_model.encoder` (`strict=False`)
- Идея: признаки, полезные для сегментации зданий, должны помогать и задаче оценки масштаба (по характерным структурам и деталям).

---

### Функция потерь и метрики
- Loss: **SmoothL1 / Huber** по `log(GSD)` (`beta=0.05`)  
  Устойчивее MSE при широком диапазоне масштабов.
- Метрики считаются уже в **м/пикс** (после `exp()`):
  - **MAE (m/px)**
  - **RMSE (m/px)**
  - **MAPE (%)** = mean(|err| / true)

---

### Обучение
- Оптимизатор: `AdamW(weight_decay=1e-4)` с разными LR:
  - encoder: **3e-6** (минимальный, чтобы не “сломать” перенесённые признаки)
  - head: **3e-5**
- Mixed precision (AMP) + gradient accumulation:
  - `batch=8`, `accum=4` → эффективный batch = **32**
- Gradient clipping: `max_grad_norm=1.0`
- DataLoader:
  - `num_workers=2`, `pin_memory=True` (на CUDA), `persistent_workers=True`
  - `prefetch_factor=2`
  - `worker_init_fn` фиксирует seed и ограничивает потоки (ускорение/стабильность)

---

### Чекпойнты и резюмирование
- Сохранение:
  - `best_gsd.pt` — лучший по **MAE на val**
  - `last_gsd.pt` — последний чекпойнт
- Resume:
  - настраивается флагом `RESUME_FROM = "last" | "best"`
  - восстанавливаются `model_state_dict`, `optimizer_state_dict`, `scaler_state_dict` (если возможно)

---

### Контроль распределения масштаба (важно для sanity check)
- Добавлен быстрый просмотр распределения `GSD` в train/val:
  - строятся гистограммы и выводятся статистики (min/p05/median/p95/max)
- Это позволяет убедиться, что scale jitter действительно покрывает нужный диапазон.

---

### Визуальная и статистическая проверка качества
Добавлены интерактивные инструменты (ipywidgets):

1) **Просмотр батча (best/last):**
- выводит N изображений из val и подписи:
  - `true GSD`, `pred GSD`, `abs_err`, `rel_err%`

2) **Гистограммы ошибок:**
- распределение абсолютной ошибки (m/px)
- распределение относительной ошибки (%)
- печать summary (MAE/RMSE/MAPE + перцентили)
- доля сэмплов, где `abs_err <= {0.01, 0.02, 0.03, 0.05} m/px`

3) **Расширенная оценка:**
- scatter `pred vs true`
- зависимость `abs_err` от `true GSD`
- доп. метрика в log-пространстве:
  - `log-MAE` и оценка типичной мультипликативной ошибки `exp(log_mae)-1`

---

### Результаты
- В этом ноутбуке качество фиксируется метриками **MAE/RMSE/MAPE** на валидации.
- Лучший чекпойнт выбирается по **минимальному MAE** (m/px).




