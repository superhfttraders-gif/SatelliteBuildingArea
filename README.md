# SatelliteBuildingArea
Определение площади застройки по спутниковому снимку.

Внешние ссылки:

Best unet: https://drive.google.com/file/d/18hq5dogLmAIZMis4aclQnHFMQKvsdXQA/view?usp=drive_link

Best gsd cnn: https://drive.google.com/file/d/1B46eXYg7HCFaDYPq7KS-BrtoRphG2W-r/view?usp=drive_link

Размеченный тестовый датасет: https://drive.google.com/file/d/15Ui39XTTtHIKwrnErSKPvl2DdAvmvq-b/view?usp=drive_link

## Постановка задачи

Цель проекта — **определение площади застройки в квадратных метрах по спутниковому снимку**.  
Для этого требуется решить две подзадачи:

1) **Сегментация зданий** → получить маску зданий и площадь застройки **в пикселях**.  
2) **Определение масштаба (GSD, meters per pixel)** → перевести площадь из пикселей в **м²**.

---

## Сегментация зданий

Для получения площади застройки в пикселях была обучена модель **бинарной сегментации зданий** по спутниковым TIFF (маска 0/1).

- Данные читались “окнами” через `rasterio`, патчи формировались **на лету** (размер патча **256×256**) с разбиением **train/val = 85/15** (`seed=42`).
- В train использовались аугментации **flip**, **rot90** и **scale jitter 0.5–1.5**; изображения нормализовались в **[0,1]**, маска бинаризовалась **0/255 → 0/1**.
- Базовая модель: `DeepLabV3-ResNet50` (torchvision) с выходом в 1 канал логитов; функция потерь: `BCEWithLogitsLoss(pos_weight=5.0) + DiceLoss` для компенсации дисбаланса классов и оптимизации перекрытия.
- Далее была протестирована архитектура **U-Net с энкодером ResNet50**, при этом энкодер инициализировался весами из backbone лучшего чекпойнта DeepLabV3 (transfer learning), что позволило улучшить качество сегментации.
- Для инференса подбирался порог бинаризации по валидации сканом порогов: порог выбирался не только по IoU, но и по близости **площади Pred к GT**, т.к. целевая метрика проекта — корректная площадь застройки.

Дополнительно была добавлена аугментация, имитирующая **окклюзии кронами деревьев** (полупрозрачные “пятна” поверх изображения, чаще накладываемые на область зданий), чтобы повысить устойчивость сегментации на реальных сценах.

## Ключевая задача точность определения масштаба (GSD)

Итоговая площадь в квадратных метрах вычисляется из площади маски в пикселях и масштаба (GSD):

`S_m2 = S_px * (GSD^2)`

Из формулы видно, что ошибка в GSD влияет на площадь **квадратично**, поэтому при прочих равных именно **минимизация ошибки определения масштаба** сильнее всего улучшает качество итоговой оценки площади застройки. По этой причине задаче определения масштаба было уделено больше времени и экспериментов.

## Определение масштаба

### Метод опорных объектов
Для определения масштаба методом опорных объектов было принято использовать модель детекции **YOLO 26x OBB**. За опорный объект принято взять **легковой автомобиль**, как наиболее часто встречающийся и при этом достаточно единообразный по размерам объект. Для учета ширины и длины одновременно за «размер авто» принято значение **квадратного корня из произведения длины и ширины BBOXа**.

Для разработки стратегии определения масштаба, обучающая выборка датасета была обработана моделью YOLO на сетке масштабов от **0.1 до 0.7 GSD**, результаты были собраны в таблицу для дальнейшего анализа.

В результате анализа полученных данных было выявлено несколько закономерностей:

- На разных ресайзах одного и того же исходного изображения ошибка в определения масштаба может быть разной, но явных тенденций в разумном диапазоне GSD выявлено не было.  
  **Вывод:** Принято использовать два дополнительных ресайза поступающего в модель изображения для увеличения стабильности. На практике оценка по трем ресайзам дала некоторые улучшения распределения ошибки, но по причине большей вычислительной сложности в итоговом продукте от нее было принято отказаться.

- На оригинальном масштабе 0,3 GSD, BBOXы авто после перевода в метры оказываются меньше, чем на любых ресайзах. И апскейл и даунскейл создают артефакты ресайза и модель начинает раздувать боксы.  
  **Вывод:** Принято принудительно выполнять апскейл входного изображения на **2%** с последующей обратной корректировкой, чтобы раздутие боксов было равномерно и независимо от того, пришел ли на вход чистый или уже ресайзнутый снимок.

- Выявлена устойчивая зависимость медианных размеров авто от общего количество авто найденных на сцене. Вероятно это связано с тем, что на городских сценах (больше машин) меньше крупных авто и часто встречаются малолитражки. На сельских/лесных сценах (меньше машин) реже встречаются малолитражки и чаще внедорожники/пикапы.  
  **Вывод:** принято использовать **регрессию предполагаемого размера авто в метрах от количества найденных авто**.

- Выявлено, что на снимках европейских населенных пунктов Вена и Тироль автомобили в среднем меньше, чем в США. При этом закономерность «автомобили в сельской местности (Тироль) больше чем в городской (Вена)» сохраняется.  
  **Вывод:** для большей устойчивости метода определения масштаба требуется **доменная адаптация** и отдельные формулы для разных стран. В данном проекте это не выполнялось.

- Выявлено что на крупных снимках 0,1-0,5 GSD точность определения масштаба в среднем выше чем на мелких 0,6-0,7. Это связано что тем, что на мелких снимках размеры объектов в пикселях становятся достаточно малы и погрешность размеров BBOX на ту же величину в пикселях дает большую погрешность в %. Также на мелких снимках задача детекции объектов в целом усложняется.  
  **Вывод:** для оценки степени надежности полученного масштаба можно использовать **количество объектов найденное на определенную площадь**, на основе которых выполнялось определение масштаба и **само значение предсказанного масштаба**.

### Метод на основе CNN
В качестве второй модели определения масштаба снимка выбрана модель на основе **сверточной сети**. В качестве базы для построения CNN под эту задачу был взят энкодер **ResNet50**, предварительно обученный на задачу **сегментации зданий** на том же датасете в рамках решения задачи определения площади строений в пикселях. К нему была пристроена регрессионная голова и проведено обучение на различных масштабах снимков из датасета (от **0,2 до 1,5 GSD**), полученных путем ресайза исходных снимков.

Была проведен эксперимент оценки масштаба только на тайлах, в которых присутствуют маски зданий. Усреднение результатов по этим тайлам дало более лучшее качество, чем по всем тайлам снимка.

Также были выявлены закономерности устойчивости модели в зависимости от процента площади снимка приходящегося на маски зданий и от предсказанного масштаба GSD. Данные предикторы можно использовать для вычисления "степени доверия" предсказанию модели при блендинге с моделью опорных объектов (в данной работе блендинг не выполнялся).

---

## Эксперимент 1: сегментация зданий на спутниковых снимках (DeepLabV3)

Ноутбук: 1. Buildings_ResNet.ipynb

**Цель:** обучить модель бинарной сегментации зданий (маска 0/1) по спутниковым TIFF.

### Данные и разбиение
- Структура датасета: `dataset/images/*.tif` и `dataset/gt/*.tif` (маски с теми же именами файлов).
- Разбиение: train/val = 85/15 (фиксированный `seed=42`).

### Пайплайн данных
- Чтение больших TIFF “окнами” (`rasterio`), генерация случайных патчей на лету.
- Размер патча: **256×256**.
- Train: **scale jitter 0.5–1.5** + аугментации (**flip**, **rot90**).
- Val: без scale jitter и без аугментаций.
- Нормализация изображения в диапазон **[0, 1]**; маска бинаризуется **0/255 → 0/1**.

### Модель
- `DeepLabV3-ResNet50` (torchvision), `num_classes=1` (логиты для бинарной сегментации).

### Функция потерь
- `BCEWithLogitsLoss(pos_weight=5.0)` + `DiceLoss` (компенсация дисбаланса классов и оптимизация перекрытия).

### Обучение
- Оптимизатор: `AdamW(lr=lr 2e-4, weight_decay=1e-4)` (после выхода на плато дообучение на пониженном LR 5e-5).
- Mixed precision (AMP) + gradient accumulation: `batch=8`, `accum=4` (эффективный batch = 32).
- Gradient clipping: `max_grad_norm=1.0`.
- Resume и сохранение чекпойнтов: `best_model.pt` (лучший по IoU) и `last_model.pt`.

### Метрики
- Precision, Recall, IoU, F1 (по пикселям) после пороговой бинаризации вероятностей.

### Результаты
- На валидации в процессе дообучения: **IoU ~ 0.72–0.74**, **F1 ~ 0.83–0.85**.
- Лучший сохранённый чекпойнт: `best_iou ≈ 0.749` (epoch 47).

### Подбор порога для инференса
- Полный скан по порогам на val: лучший IoU при **thr = 0.78**  
  **IoU = 0.7413**, **P = 0.834**, **R = 0.870**, **F1 = 0.8514**.
- Проверка площади на val при `thr=0.78`: **Pred/GT ≈ 1.043** (≈ +4.3% площади относительно GT).
- **Лучший порог по площади (минимизация |Pred−GT|):** среди протестированных значений наиболее близкое совпадение площади даёт **thr = 0.80**  
  (Δ_area ≈ **+382,957 px**, **Pred/GT ≈ 1.026** на скане; площадь ближе к GT, но IoU практически не меняется: **IoU ≈ 0.7412**).

### Визуальная проверка
- Добавлен интерактивный просмотр (ipywidgets): выбор `best/last`, настройка порога, визуализация `GT / Pred / Overlay` + сравнение площадей.

## Эксперимент 2: U-Net с энкодером ResNet50 (transfer из DeepLabV3)

Ноутбук: 2. Buildings_UNet.ipynb

**Цель:** попробовать альтернативную архитектуру (U-Net) для бинарной сегментации зданий и улучшить качество относительно DeepLabV3 за счёт переносa признаков энкодера.

### Данные и пайплайн
- Используется тот же датасет `images/*.tif` + `gt/*.tif`, разбиение train/val = 85/15 (`seed=42`).
- Патчи **256×256**, чтение TIFF “окнами” (`rasterio`).
- Train: `scale jitter 0.5–1.5` + аугментации (flip, rot90).  
- Val: без jitter/аугментаций.
- Нормализация входа в **[0, 1]**, маска **0/255 → 0/1**.

### Модель
- **U-Net** с энкодером **ResNet50** (torchvision).  
- Выход: 1 канал логитов (`{"out": logits}`), чтобы совпасть с существующим training loop.

### Transfer learning (инициализация энкодера)
- Энкодер U-Net инициализируется весами из лучшего чекпойнта **DeepLabV3**:
  - из state_dict берутся ключи с префиксом `backbone.*` и загружаются в `model.encoder`.
- Декодер и голова U-Net — новые (обучаются с нуля).

### Функция потерь и метрики
- Loss: `BCEWithLogitsLoss(pos_weight=5.0)` + `DiceLoss`.
- Метрики: Precision / Recall / IoU / F1 (по пикселям), порог по умолчанию **thr=0.80**.

### Обучение
- `AdamW(weight_decay=1e-4)` с разными LR:
  - encoder: **1e-5** (очень маленький, чтобы не “сломать” признаки), после достижения плато снижаем до **3e-6**,
  - decoder/head: **1e-4** (выше, чтобы быстрее обучить декодер), после достижения плато снижаем до **3e-5**.
- AMP + gradient accumulation: `batch=8`, `accum=4` (эффективный batch=32).
- Gradient clipping: `max_grad_norm=1.0`.
- Resume из `/content/drive/MyDrive/Models/last_unet.pt`, сохранение `best_unet.pt` и `last_unet.pt`.

### Результаты (валидация)
- Лучший сохранённый чекпойнт: `best_iou ≈ 0.755` (epoch 50).

### Подбор порога для инференса
- Полный скан порогов на val показал лучший IoU при **thr = 0.80**:  
  **IoU = 0.7483**, **P = 0.842**, **R = 0.870**, **F1 = 0.8560**.
- **Лучший порог по площади (минимизация |Pred−GT| среди протестированных):** также **thr = 0.80**  
  (на скане Δ_area = **+467,899 px**, **Pred/GT = 1.033**).
- Итоговая проверка площади по всему val при `thr=0.80`: **Pred/GT ≈ 1.057** (≈ +5.7% площади относительно GT).

### Визуальная проверка
- Добавлен интерактивный просмотр (ipywidgets): выбор `best/last`, настройка порога, визуализация `GT / Pred / Overlay` + сравнение площадей.

## Эксперимент 3: Новая аугментация: имитация окклюзий “кронами деревьев”

Ноутбук: 3. Buildings_UNet_(trees).ipynb

- В `CropConfig` добавлены параметры `tree_*`, и в `_augment()` включена опция **tree occlusion**.
- Реализована функция `add_tree_blobs()`: поверх изображения накладываются полупрозрачные “пятна” (зелёные/иногда чёрные), сформированные из набора размытых кругов + шумовой текстуры.
- Центры пятен с вероятностью `tree_on_mask_prob` выбираются **по пикселям здания** (то есть окклюзия чаще закрывает здания, а не фон).

Параметры для train:
- `tree_p=0.30` (30% патчей с окклюзией)
- 2–6 пятен на патч (`tree_blobs`)
- радиусы 3–15 px (`tree_r_px`)
- непрозрачность 0.70–1.00 (`tree_alpha`)
- небольшое размытие `k=3..7` (`tree_blur_k`)
- `tree_black_prob=0.20`

Валидация: `tree_p=0.0` (без окклюзий).

### Порог для метрик и инференса
- Базовый порог в `metrics_from_logits()` и в визуализации изменён на **thr=0.86** (в прошлом ноутбуке был 0.80).
- Скан порогов смещён в “верхнюю” область: теперь тестируются **0.70–0.94** с шагами (0.70, 0.75, 0.80, 0.82, 0.84, 0.86, 0.88, 0.90, 0.92, 0.94).

### Результаты подбора порога (best U-Net)
- Лучший IoU на full val при **thr = 0.84**:  
  **IoU = 0.7581**, **P = 0.854**, **R = 0.871**, **F1 = 0.8624**.
- Лучший порог по площади (минимизация |Pred−GT| среди протестированных) — **thr = 0.86**:  
  на скане Δ_area = **−13,048 px**, **Pred/GT = 0.999** (практически идеальное совпадение площади).
- Полная проверка площади при `thr=0.84`: **Pred/GT ≈ 1.035** (≈ +3.5% площади относительно GT).

### Чекпойнты и прогресс обучения
- Продолжение обучения шло из `last_unet.pt` 
- После эпохи epoch=72 (best_iou ≈ 0.7612).

## Эксперимент 4: Регрессия GSD (оценка масштаба/разрешения) CNN моделью по патчам со спутниковых снимков

Ноутбук: 4. Buildings_UNet_(Scale) (mask only).ipynb

**Цель:** обучить модель, которая по изображению патча предсказывает **GSD (meters per pixel)**, чтобы далее корректно пересчитывать площадь зданий в физические единицы при разном масштабе/зуме/ресэмплинге.

### Данные и разбиение
- Используется тот же датасет сцен:
  - изображения: `dataset/images/*.tif`
  - маски зданий: `dataset/gt/*.tif` (те же имена файлов)
- Разбиение: train/val = **85/15** (`seed=42`).
- **Фильтрация пустых патчей:** при формировании выборки **отбрасываются** окна, где в маске нет зданий (`mask window sum == 0`).  
  Это важно, чтобы модель училась по информативным структурам (здания/контуры), а не по “пустому фону”.

### Пайплайн данных (GSD-датасет)
- Чтение больших TIFF “окнами” через `rasterio`, генерация патчей **на лету**.
- Размер выходного патча: **256×256**.
- Вводится “виртуальный zoom” **z**, который имитирует разные разрешения:
  - `z ~ U_log(0.25, 1.5)` (log-uniform sampling)
  - исходный размер окна: `src_sz ≈ patch_size / z`, затем ресайз обратно в 256×256
  - целевой GSD патча: `GSD = GSD0 / z`, где `GSD0 = 0.30 м/пикс` (базовый GSD тренировочного датасета)
- Таргет для регрессии: **log(GSD)** (стабильнее, мультипликативная шкала).
- Нормализация входа: изображение приводится к **[0, 1]**, поддерживаются `uint8/uint16/float` TIFF.

### Аугментации (anti-cheat, для устойчивости к “реальному миру”)
В train включены аугментации, которые имитируют разные источники данных и деградации:

- Геометрия: `flip`, `rot90`.
- Photometric jitter (с вероятностью `color_p=0.85`):
  - gamma: `0.85–1.15`
  - brightness/contrast/saturation: `±0.12`
- Blur: `p=0.20`, `sigma 0.2–1.0`
- Sharpen (unsharp mask): `p=0.12`, `amount 0.2–0.6`, `sigma 0.6–1.4`
- Noise: `p=0.30`, `std 0.0–0.02`
- JPEG-like деградация: `p=0.30`, квантование уровней (`q 32–128`) + downscale `0.6–1.0`
- “Shade” (мягкое затенение): `p=0.20`, сила `0.02–0.10`

Валидация:
- **scale jitter включён**, но **augment выключен** (чистая оценка, кроме изменения масштаба).

### Модель
- **ResNet50 encoder → регрессор GSD**
- Архитектура:
  - backbone: `resnet50(weights=None)` (fc удалён)
  - GAP (global average pooling) → MLP head:
    - `2048 → 256 → 1`
    - `Dropout=0.2`
- Выход: `{"t": pred_log_gsd}`, где `t = log(GSD)`.

### Transfer learning (инициализация энкодера из U-Net)
- Энкодер ResNet50 **инициализируется** из чекпойнта U-Net (из экспериментов 2–3):
  - из state_dict берутся ключи `encoder.*` (fallback: `model.encoder.*`)
  - загружаются в `gsd_model.encoder` (`strict=False`)
- Идея: признаки, полезные для сегментации зданий, должны помогать и задаче оценки масштаба (по характерным структурам и деталям).

### Функция потерь и метрики
- Loss: **SmoothL1 / Huber** по `log(GSD)` (`beta=0.05`)  
  Устойчивее MSE при широком диапазоне масштабов.
- Метрики считаются уже в **м/пикс** (после `exp()`):
  - **MAE (m/px)**
  - **RMSE (m/px)**
  - **MAPE (%)** = mean(|err| / true)

### Обучение
- Оптимизатор: `AdamW(weight_decay=1e-4)` с разными LR:
  - encoder: **3e-6** (минимальный, чтобы не “сломать” перенесённые признаки)
  - head: **3e-5**
- Mixed precision (AMP) + gradient accumulation:
  - `batch=8`, `accum=4` → эффективный batch = **32**
- Gradient clipping: `max_grad_norm=1.0`
- DataLoader:
  - `num_workers=2`, `pin_memory=True` (на CUDA), `persistent_workers=True`
  - `prefetch_factor=2`
  - `worker_init_fn` фиксирует seed и ограничивает потоки (ускорение/стабильность)

### Чекпойнты и резюмирование
- Сохранение:
  - `best_gsd.pt` — лучший по **MAE на val**
  - `last_gsd.pt` — последний чекпойнт
- Resume:
  - настраивается флагом `RESUME_FROM = "last" | "best"`
  - восстанавливаются `model_state_dict`, `optimizer_state_dict`, `scaler_state_dict` (если возможно)

### Контроль распределения масштаба (важно для sanity check)
- Добавлен быстрый просмотр распределения `GSD` в train/val:
  - строятся гистограммы и выводятся статистики (min/p05/median/p95/max)
- Это позволяет убедиться, что scale jitter действительно покрывает нужный диапазон.

### Визуальная и статистическая проверка качества
Добавлены интерактивные инструменты (ipywidgets):

1) **Просмотр батча (best/last):**
- выводит N изображений из val и подписи:
  - `true GSD`, `pred GSD`, `abs_err`, `rel_err%`

2) **Гистограммы ошибок:**
- распределение абсолютной ошибки (m/px)
- распределение относительной ошибки (%)
- печать summary (MAE/RMSE/MAPE + перцентили)
- доля сэмплов, где `abs_err <= {0.01, 0.02, 0.03, 0.05} m/px`

3) **Расширенная оценка:**
- scatter `pred vs true`
- зависимость `abs_err` от `true GSD`
- доп. метрика в log-пространстве:
  - `log-MAE` и оценка типичной мультипликативной ошибки `exp(log_mae)-1`

### Результаты
- В этом ноутбуке качество фиксируется метриками **MAE/RMSE/MAPE** на валидации.
- Лучший чекпойнт выбирается по **минимальному MAE** (m/px).

## Эксперимент 5: Инференс-стресс-тест CNN модели определения GSD на больших сценах со скользящим окном и агрегацией (mean vs median)

Ноутбук: 5. ScaleTest.ipynb

**Цель:** проверить, как модель **GSD-регрессии** (эксп. 4) ведёт себя в реалистичном сценарии инференса:
- берём **целую сцену** (val), 
- искусственно меняем “разрешение” (масштаб) **ресайзом всего изображения и маски**,
- режем на перекрывающиеся **256×256 окна**, оставляем только окна **с зданиями**,
- предсказываем GSD по каждому окну и агрегируем по сцене,
- сравниваем оценку с **истинным GSD после ресайза**.

### Данные и разбиение
- Используется val-разбиение того же датасета:
  - `dataset/images/*.tif`
  - `dataset/gt/*.tif`
- Разбиение: **train/val = 85/15** (`seed=42`).
- В эксперименте прогоняются **все val-сцены**: `n_images = 27`.

### Постановка эксперимента (симуляция разных GSD)
- Базовый масштаб датасета: `GSD0 = 0.30 м/пикс`.
- Для каждой val-сцены прогоняется набор **N_SCALES = 10** “разрешений”, равномерно по **истинному GSD**:
  - `GSD_true ∈ [0.30 .. 1.20] м/пикс` (шаг ≈ 0.10)
  - Для каждого `GSD_true` вычисляется `z = GSD0 / GSD_true`
  - Ресайз всей сцены и маски одним и тем же `z`
    - если `z < 1`: `area` (downscale)
    - если `z > 1`: `bicubic` (upscale, antialias=True где возможно)
- Таким образом симулируется, что исходная сцена “приходит” с другим GSD, а модель должна его восстановить.

### Пайплайн инференса (sliding windows)
- Патч: **256×256**
- Шаг: **stride = 128** (50% overlap)
- Для каждого окна:
  - берём соответствующее окно маски
  - **оставляем окно только если есть здания** (`(mask > 0).sum() >= 1`)
- Батч-инференс: `batch_size=32`
- Предсказания:
  - модель выдаёт `t = log(GSD_hat)`
  - переводим в метры/пикс: `GSD_hat = exp(clamp(t, -10, 10))`

### Модель
- Та же модель, что в эксперименте 4:
  - `ResNet50GSDRegressor`
- Чекпойнт: `best_gsd.pt`

### Агрегация предсказаний по сцене
Для каждой сцены и каждого масштаба получаем набор оконных предсказаний `GSD_hat_i` и считаем:

- `GSD_scene_mean = mean(GSD_hat_i)` (стримингово, без хранения всех значений)
- `GSD_scene_median = median(GSD_hat_i)` (хранение ограничено `MEDIAN_MAX_STORE = 200k`, чтобы не взорвать память)

Далее ошибки относительно истинного GSD после ресайза:

- `abs_err = |GSD_scene - GSD_true|`
- `rel_err_pct = abs_err / GSD_true * 100%`

### Метрики и отчёты
Собирается таблица `image × scale` с колонками:
- `gsd_true`, `z`, `(newW,newH)`, метод ресайза
- `kept_building_windows`, `pred_count`
- `gsd_mean`, `rel_err_mean_pct`
- `gsd_median`, `rel_err_median_pct`

Дополнительно строятся агрегаты:
- **по сценам** (ошибка по масштабам)
- **по “группам” сцен** (по префиксу имени: kitsap/chicago/vienna/austin/tyrol)
- **по scale_idx** (как меняется ошибка при ухудшении/улучшении GSD)
- гладкие поверхности ошибок через **NW kernel regression**:
  - `E[rel_err | predicted_gsd]`
  - `E[rel_err | predicted_gsd, mask_frac]`, где `mask_frac` — доля пикселей зданий на исходной маске сцены

## Результаты

### 1) Группы сцен (медианная ошибка по масштабам)
Агрегация по префиксу имени файла (robust-оценка через median по масштабам):

| group   | n_images | rel_err_median_median (%) | rel_err_mean_median (%) | preds_total |
|---------|----------|----------------------------|--------------------------|------------|
| kitsap  | 2        | 4.170473                   | 2.724372                 | 6,559      |
| chicago | 7        | 4.523442                   | 5.621863                 | 25,715     |
| vienna  | 7        | 4.735178                   | 5.781111                 | 25,343     |
| austin  | 6        | 5.250098                   | 5.631460                 | 18,919     |
| tyrol   | 5        | 7.492296                   | 7.120409                 | 11,390     |

**Наблюдения:**
- В среднем **median-агрегация** по окнам часто выигрывает у mean (особенно когда появляются “выбросы” по окнам).
- Есть исключения (например, kitsap и tyrol), где mean чуть лучше — это может зависеть от структуры сцены, доли зданий и характера шумов.

### 2) Ошибка по истинному масштабу (scale_idx = 1..10, GSD_true ≈ 0.30..1.20)
Сводка по всем val-сценам (27 сцен на каждый scale_idx), показана **медиана относительной ошибки** по сценам на данном масштабе:

| scale_idx | preds_total | rel_err_mean_med (%) | rel_err_median_med (%) |
|----------:|------------:|----------------------:|------------------------:|
| 1  (≈0.30 m/px) | 29134 | 11.6229 | 10.1900 |
| 2  (≈0.40 m/px) | 17224 | 3.8252  | 4.7052  |
| 3  (≈0.50 m/px) | 11403 | 4.8991  | 5.3313  |
| 4  (≈0.60 m/px) | 8050  | 4.8509  | 4.0401  |
| 5  (≈0.70 m/px) | 5905  | 4.5900  | 3.8542  |
| 6  (≈0.80 m/px) | 4636  | 3.8395  | **3.2333** |
| 7  (≈0.90 m/px) | 4076  | 5.2516  | 4.0115  |
| 8  (≈1.00 m/px) | 2962  | 6.1951  | 5.3276  |
| 9  (≈1.10 m/px) | 2495  | 8.6396  | 7.1803  |
| 10 (≈1.20 m/px) | 2041  | 12.9782 | 12.8612 |

**Ключевой вывод по масштабу:** ошибка имеет выраженную **U-образную** зависимость:
- хуже всего на “краях” (по GSD: ~0.30 и ~1.20),
- лучший диапазон — примерно **0.6–0.8 м/пикс** (scale_idx 4–6), где достигается минимум медианной ошибки (около **3–4%**).

Также видно, что при ухудшении разрешения (к большим GSD) резко падает `preds_total` (меньше окон) — и это добавляет нестабильности сценовой оценке.

### 3) Непрерывный анализ ошибок (kernel regression)
Добавлен анализ гладких зависимостей ошибки:

- **E[rel_err | predicted_gsd]** (1D): минимум ожидаемой ошибки лежит примерно в зоне **~0.6–0.8 m/px** по предсказанному GSD.
- **E[rel_err | predicted_gsd, mask_frac]** (2D): ошибка заметно зависит от доли зданий в сцене:
  - при очень маленьком `mask_frac` (мало “сигнала”) и на краях по GSD ошибка выше;
  - средние значения `mask_frac` дают наиболее стабильные оценки.

### Визуальная проверка / диагностика
- Таблицы:
  - `df_detail`: все строки `image × scale`
  - `df_img_summary`: агрегаты по сценам
  - `scale_summary`: агрегаты по scale_idx
- Графики:
  - `E[rel_err | predicted_gsd]` (1D)
  - heatmap `E[rel_err | predicted_gsd, mask_frac]` (2D)

### Итог
Этот эксперимент подтверждает, что GSD-регрессор **работает устойчиво**, а для агрегации по окнам чаще всего выгоднее **median**, чем mean. Ошибка существенно зависит от масштаба: наиболее стабильная зона — **средние GSD (~0.6–0.8 м/пикс)**, тогда как на крайних разрешениях точность падает.

## Эксперимент 6: Определение масштаба через детекцию машин (YOLO-OBB) + multi-GSD “обучающая” прогонка + адаптивный conf + 3-scale inference

Ноутбук: 6. Multi_scale_inference (+filters).ipynb

**Цель:** оценивать масштаб (GSD, м/пикс) сцены, используя детектор машин с ориентированными боксами (OBB).  
Идея: у легковой машины есть “типичный” размер в метрах, а значит по размеру детекций в пикселях можно восстановить GSD.

### Данные и разбиение
- Используется тот же набор изображений `dataset/images/*.tif` (маски зданий здесь не используются).
- Разбиение: **train/val = 85/15** (`seed=42`).
- В этом ноутбуке прогон выполняется **только по train-сценам**, чтобы собрать статистики по множеству масштабов.

### Основная идея (GSD через “типичную машину”)
- Базовый GSD датасета: `GSD_BASE = 0.30 м/пикс`.
- Для каждой сцены создаётся несколько версий изображения, ресайзом под целевые масштабы:
  - `GSD_TARGETS = [0.10, 0.20, 0.29, 0.40, 0.50, 0.60, 0.70]`
  - ресайз-фактор: `resize_factor = GSD_BASE / GSD_TARGET`
  - upsample → `cv2.INTER_CUBIC`, downsample → `cv2.INTER_AREA`
- На ресайзнутой сцене детектятся машины; по статистике размеров OBB строится оценка масштаба:
  - для каждого бокса: `L=max(w,h)`, `W=min(w,h)`, `sqrtLW = sqrt(L*W)` (в пикселях)
  - предполагаем “типичный” метрический размер:
    - `TYPICAL_CAR_SQRTLW_M = 3.2` метра (это √(L·W) в метрах для средней машины)
  - грубая оценка:
    - `GSD_est ≈ TYPICAL_CAR_SQRTLW_M / median(sqrtLW_px)`

### Детектор
- Модель: **Ultralytics YOLO OBB**
- Веса: `yolo26x-obb.pt`
- Класс машин: `CAR_CLASS_ID = 10`
- Инференс:
  - `half=True`
  - `iou=0.5` (tile-NMS)
  - `conf` — адаптивный (см. ниже)

### Тилинг больших сцен + батчинг тайлов
Чтобы обрабатывать большие TIFF эффективно:
- Тайлы: `TILE_SIZE = 1024`
- Overlap: `OVERLAP = 0.1`
- Тайлы пакуются батчами: `TILE_BATCH = 16`
- Для каждого тайла выполняется `model.predict(source=[tile1, tile2, ...])`, затем координаты боксов переводятся в глобальные.

### Дедупликация детекций между тайлами
- Быстрое “слияние дублей” по хешу центра:
  - клетка: `MERGE_CELL_PX = 3`
  - для ближайших клеток выбирается детекция с максимальным `conf`
- Это грубее полноценного геометрического NMS по OBB, но **очень быстро** и достаточно для статистики размеров.

### Фильтрация “car-like” боксов (качество статистики)
Чтобы статистики размеров не ломались на мусорных детекциях:

1) **Фильтр по соотношению сторон** (машина должна быть вытянутой, но не “иголкой”):
- `ASPECT_RATIO_MIN = 1.2`
- `ASPECT_RATIO_MAX = 3.8`
- критерий: `ratio = max(w,h)/min(w,h)`

2) **IQR-фильтр по sqrt(L·W)** для выбросов:
- считаем `s = sqrt(L*W)` по всем детекциям
- оставляем в диапазоне `[Q1 - 1.5*IQR, Q3 + 1.5*IQR]`
- включается только если детекций достаточно (`len >= 6`)

### Адаптивный порог confidence (CONF_PRIMARY / CONF_FALLBACK)
Чтобы не “падать” на сценах, где машин мало или они маленькие:

- Основной прогон: `CONF_PRIMARY = 0.10`
- Если после фильтров **машин недостаточно**, выполняется fallback:
  - `CONF_FALLBACK = 0.01`

Порог достаточности:
- для primary: `MIN_CARS_FOR_ANALYSIS_1 = 6`
- для fallback: `MIN_CARS_FOR_ANALYSIS_2 = 4` (важно: не 1, чтобы не принимать единичный мусор)

Если и fallback не набрал минимум — в таблицу пишутся NaN (явный “fail”).

### 3-scale inference (подбор масштабов вокруг оценки)
После базового детекта на масштабе 1.0 (на самом деле “base” здесь — текущая ресайзнутая под GSD_TARGET сцена):

1) Оцениваем `GSD_est` по `median_sqrtLW_px`.
2) Выбираем ещё 2 масштаба (помимо 1.0), чтобы:
   - либо увеличить машины (если они слишком мелкие),
   - либо уменьшить (если слишком крупные),
   - либо расширить диапазон, когда оценка неуверенная.

Правило выбора `choose_three_scales(gsd_est)`:

- `gsd_est <= 0.20`  → `[1.0, 0.5, 0.3]`
- `(0.20, 0.35]`     → `[1.0, 0.5, 2.0]`
- `(0.35, 0.50]`     → `[1.0, 0.7, 2.0]`
- `> 0.50`           → `[1.0, 1.5, 2.5]`
- если `gsd_est` NaN → `[1.0, 0.5, 2.0]`

Для scale2/scale3:
- изображение ресайзится (`resize_by_factor`)
- детекции прогоняются через те же фильтры (aspect ratio + IQR)
- вычисляются **median и mean** размеров (L, W, sqrtLW)

**Нормализация обратно к “base scale”:**
- все размеры в пикселях делятся на `factor`, чтобы сравнивать между scale1/2/3 в общей системе:
  - `value_norm_px = value_px / scale_factor`

### Что сохраняется (выходные таблицы)
На выход пишется один ряд на комбинацию `(image × gsd_target)`:

- идентификаторы:
  - `image_name`
  - `gsd_m_per_px` (целевой GSD, под который ресайзили)
  - `resize_factor`
- параметры тайлинга:
  - `tile_size`, `overlap`, `tile_batch`
- адаптивное решение:
  - `conf_primary`, `count_primary`
  - `conf_fallback`, `count_fallback`
  - `conf_used`, `count_used`
- оценка масштаба:
  - `gsd_est_m_per_px` (по median sqrtLW на scale1)
- выбранные масштабы:
  - `scale_1`, `scale_2`, `scale_3`
- статистики размеров (в пикселях, нормированные к base scale):
  - `med_len_s{1,2,3}_px`, `med_wid_s{1,2,3}_px`, `med_sqrtLW_s{1,2,3}_px`
  - `mean_len_s{1,2,3}_px`, `mean_wid_s{1,2,3}_px`, `mean_sqrtLW_s{1,2,3}_px`
  - `count_s{1,2,3}`

Файлы:
- финал:
  - `train_vehicle_stats_adaptive_3scale_pixels.csv`
  - `train_vehicle_stats_adaptive_3scale_pixels.xlsx`
- чекпойнты каждые `CHECKPOINT_EVERY_IMAGES = 20` сцен:
  - `..._PARTIAL.csv / ..._PARTIAL.xlsx`

Численные метрики качества (ошибка по GSD относительно истинного) в этом ноутбуке не считаются, т.к. он формирует датасет статистик. Оценка точности выполнена отдельным шагом.

## Эксперимент 7: Анализ статистик детектора машин (YOLO-OBB) и калибровка оценки GSD по “типичной машине”

Ноутбук: 7. YOLO_Table_Analysis.ipynb

**Цель:** проанализировать таблицу статистик, собранную в ноутбуке 6 (multi-scale inference), и построить простой/быстрый метод оценки **GSD (м/пикс)** по размерам детекций машин, включая калибровку и оценку “уверенности”.

### Данные
- Входной файл: `train_vehicle_stats_adaptive_3scale_pixels (f).xlsx`
- Таблица содержит строки вида `(image × gsd_target)` со статистиками детекций по трём скейлам (s1/s2/s3).
- Для анализа удалены служебные колонки (тайлинг, overlap, conf’ы, scale_factors и т.п.), чтобы оставить только полезные признаки/статистики.

### Связь количества детекций и масштаба
Построена агрегированная зависимость `count_used` от `gsd_m_per_px`:

| gsd_m_per_px | mean(count_used) | median(count_used) | std | count |
|---:|---:|---:|---:|---:|
| 0.10 | 1647.36 | 1488 | 1471.20 | 153 |
| 0.20 | 1772.74 | 1583 | 1593.82 | 153 |
| 0.29 | 1701.17 | 1389 | 1514.60 | 153 |
| 0.40 | 1199.27 | 1128 | 1059.14 | 153 |
| 0.50 | 690.00  | 577  | 665.07  | 153 |
| 0.60 | 380.72  | 274  | 407.66  | 153 |
| 0.70 | 68.66   | 25   | 105.64  | 153 |

**Вывод:** чем больше **GSD** (хуже разрешение, больше м/пикс), тем **меньше машин детектится** (ожидаемо: машины становятся слишком маленькими/смазанными).

### Проверка “типичных размеров” машины в метрах и доменный сдвиг
Для скейла **s1** пересчитаны статистики из пикселей в метры:

- `med_len_s1_m = med_len_s1_px * gsd_m_per_px`
- `med_wid_s1_m = med_wid_s1_px * gsd_m_per_px`
- `med_sqrtLW_s1_m = med_sqrtLW_s1_px * gsd_m_per_px`

Сводка по распределениям (s1, в метрах):
- **median length ≈ 4.7875 m**
- **median width ≈ 2.1695 m**
- **median sqrt(L·W) ≈ 3.2122 m**

(распределения не идеальны: обнаружен дополнительный “горб”, т.е. смесь режимов)

Далее выполнено разбиение сцен по “городам” по подстроке в `image_name` (`vienna/tyrol/kitsap/chicago/austin`) и построены распределения по каждому городу.

**Наблюдение:** гипотеза доменного сдвига подтверждена — **для Vienna характерны меньшие “типичные” машины** (в метрах), что даёт смещение в методе “один эталонный размер на всех”.

**Идея улучшения (в проекте не реализовано из-за сроков):**
- вводить **региональную/доменную калибровку** (по стране/городу/типу источника),
- либо учить небольшую модель, предсказывающую “типичный размер” как функцию признаков сцены/детекций.

### Эффект адаптивного порога confidence (primary/fallback)
Проверено, сколько строк имеют детекции на:
- `CONF_PRIMARY = 0.10`
- `CONF_FALLBACK = 0.01`

**Результат:**
- fallback “страхует” примерно **12.7%** случаев (детекции появляются только при пониженном conf),
- полностью “провальных” строк без машин около **30** (≈ **2.8%**).

### Стратегии выбора размера по 3 скейлам (s1/s2/s3)
Проверены варианты агрегации `sqrt(L·W)`:

1) **S1 baseline:** использовать только `med_sqrtLW_s1`
2) **Best-by-count:** брать `med_sqrtLW_s{argmax(count_s1,count_s2,count_s3)}`
3) **Weighted:** усреднять `med_sqrtLW_s1/s2/s3` с весами `count_s1/s2/s3`

По сводным статистикам наиболее “узкое” распределение давал **Best-by-count**, но выявилась проблема **правого хвоста** (редкие кейсы со значительно завышенными размерами), чаще в **kitsap/austin** на больших GSD (0.5–0.7), где детекций мало и медианы становятся шумными.

#### Правило для подавления хвоста (Best + safety-switch)
Введено правило: переключаться с s1 на s2/s3 только если одновременно:
- прирост детекций существенный: `delta_count > 5`
- и оценка размера близка к s1: `|m_sX - m_s1| / m_s1 ≤ 10%`

**Эффект:** “Best” стал стабильным без раздувания хвостов, но ценой вычислений: для надёжной стратегии нужно **от 3 проходов детектора**, что дорого по времени.

**Практический вывод:**
- для **быстрого режима** разумно использовать **S1** (один прогон),
- для **точного режима** — multi-scale с правилом переключения.

### Быстрая модель оценки GSD по S1 (через эталонный размер машины)
Построены три “однопараметрические” оценки масштаба:

- по `sqrt(L·W)`:
  - `GSD_pred = TARGET_SQRTLW_M / med_sqrtLW_s1_px`
- по ширине:
  - `GSD_pred = TARGET_W_M / med_wid_s1_px`
- по длине:
  - `GSD_pred = TARGET_L_M / med_len_s1_px`

Где эталоны взяты как медианы по train-таблице (в метрах):
- `TARGET_L_M = 4.7875`
- `TARGET_W_M = 2.1695`
- `TARGET_SQRTLW_M = 3.212`

Сравнение ошибок показало, что **sqrt(L·W)** — лучший предиктор (самый устойчивый к ориентации и форме бокса).

Ключевые метрики для `gsd_pred_sqrtlw` (ошибка в % относительно истинного `gsd_m_per_px`):
- **bias_mean ≈ +0.0435%** (почти ноль)
- **MAPE ≈ 4.594%**
- **MedAPE ≈ 3.669%**
- **RMSPE ≈ 5.867%**
- хвосты: `p95_abs ≈ 10.844%`, `max_abs ≈ 24.515%`
- доля “попаданий”: `within_5% ≈ 0.615`

### Калибровка через количество детекций (учёт эффекта масштаба и “насыщенности” сцены)
Идея: типичный “размер машины” в данных слегка меняется вместе с `count` (и вместе с GSD).
Это вероятно связано с тем, что на сельских сценах в среднем меньше детекций, а размер авто в среднем больше (пикапы, внедорожники). На городских сценах наоборот: количество авто больше, но их средний размер меньше (больше малолитражек, меньше крупных авто).

Поэтому добавлена log-калибровка с interaction:

\[
\log(gsd\_true)=a+b\log(gsd\_pred)+c\log(count)+d\log(gsd\_pred)\log(count)
\]

Оценённые коэффициенты:
- `a = 0.11032050`
- `b = 1.02631206`
- `c = -0.01989536`
- `d = -0.00519021`

Sanity-check (MAPE):
- **base:** 4.5944%
- **calibrated:** **4.3114%**

Сводка качества (после калибровки `gsd_pred_sqrtlw_calib`):
- **MAPE улучшился:** 4.594 → **4.304**
- **RMSPE улучшился:** 5.867 → **5.214**
- **хвосты улучшились:** `p95_abs 10.844 → 9.397`, `max_abs 24.515 → 19.807`
- при этом **MedAPE стал хуже** (3.669 → 3.937) и `within_1%/within_2%` слегка просели (калибровка помогает хвостам, но не всем точкам).

Дополнительно сделан небольшой **bias-shift** множителем `k ≈ 0.99863`, чтобы вернуть центр распределения ближе к нулю.

### Оценка “уверенности” предсказания (uncertainty proxy)
Построена таблица ошибок по сетке:
- `bin(gsd_pred_calib)` (квантильное биннингование по предсказанному GSD)
- `bin(log(count))` (квантильное биннингование по числу детекций)

В каждой ячейке считался:
- `RMSE(log_err)` где `log_err = log(pred) - log(true)` (мультипликативная ошибка)
- также считался `N` наблюдений в ячейке (карта заполненности)

Затем:
- выполнена **bilinear-интерполяция** `sigma_log` с центров бинов на любые `(gsd_pred, log(count))`
- переведено в проценты как:
  - `sigma_pct_approx = 100 * (exp(sigma_log) - 1)`

Проверка:
- scatter `sigma_pct_approx` vs фактическая `|err|%` показывает **положительную связь**, но с заметным разбросом (как и ожидается от простой “табличной” uncertainty).
- RMS ошибка растёт с увеличением `gsd_pred_calib` (хуже качество на больших GSD), сглажено LOWESS.

### Итог
- Табличная статистика YOLO-детекций даёт работоспособный “первый принцип” метод оценки **GSD** по **типичному размеру машины**, лучший вариант — через **sqrt(L·W)**.
- Простая калибровка через `log(count)` улучшает **MAPE и хвосты**, но не гарантирует улучшение для каждой точки (≈ 53% улучшений, ≈ 47% ухудшений).
- Для production-режимов:
  - **быстрый режим:** один прогон (S1) + формула по sqrt(L·W) (+калибровка при желании),
  - **точный режим:** multi-scale + правило безопасного переключения (дороже по времени).
- Наблюдается доменный сдвиг (Vienna), что открывает путь к улучшению через **региональную адаптацию** или **обучаемую калибровку по доменам**.
- Возможность получения оценки "степени уверенности" модели дает возможность выполнить блендинг с моделью CNN с весами пропорциональными "степени уверенности" моделей.

## Эксперимент 8: Предварительная разметка тестового датасета (MS Buildings footprints to CVAT) с уточнением геосдвига через YOLO-seg

Ноутбук: 9. Авто_разметка.ipynb

**Цель:** автоматически подготовить стартовую разметку зданий для тестового датасета из GeoTIFF сцен:
1) скачать “сырые” футпринты зданий из **Microsoft Global Buildings** для AOI сцены,  
2) выполнить **YOLOv8 segmentation** по изображению (тайлинг),  
3) сматчить футпринты и YOLO-детекции,  
4) для матченных объектов уточнить **сдвиг** футпринтов по **IoU** против YOLO-масок,  
5) экспортировать разметку в **CVAT** с двумя лейблами (**Buildings** / **ToDelete**) + сделать визуальные отчёты и архивы.

### Данные и источники
- Вход: GeoTIFF сцены в папке `INPUT_DIR = /content/tifs` (`*.tif/*.tiff`).
- Геофутпринты: **Microsoft Global Buildings** (поток GeoJSONL.GZ), ссылки берутся из:
  - `dataset-links.csv` по адресу `https://minedbuildings.z5.web.core.windows.net/global-buildings/dataset-links.csv`
- AOI определяется из bbox GeoTIFF, который переводится в WGS84 (`transform_bounds`), затем bbox покрывается тайлами на `ZOOM = 9`, получаются quadkeys (`mercantile.tiles → mercantile.quadkey`).
- Фильтр футпринтов:
  - клип по AOI bbox,
  - проверка валидности геометрии,
  - фильтр минимальной площади `MIN_AREA_M2 = 5.0`.

### Инференс YOLO (segmentation) по сцене
**Модель:**
- Ultralytics YOLOv8 сегментация, веса:
  - `yolov8m-seg_LasVegas.pt` (скачиваются по ссылке `BUILDING_MODEL_URL` в `WEIGHTS_PATH`)
- Загрузка модели сделана **robust** для PyTorch 2.6+:
  - попытка `torch.serialization.add_safe_globals(...)`,
  - fallback: `torch.load(weights_only=False)`.

**Параметры инференса:**
- `YOLO_CONF = 0.50`
- `YOLO_IOU = 0.60`
- Тайлинг:
  - `IMG_TILE = 1024`
  - `TILE_OVERLAP = 0.20`
- Ограничения:
  - `MAX_DET_PER_TILE = 400` (берём top-k по conf на тайле)
  - фильтр минимальной площади полигона `MIN_POLY_AREA_PX = 25`
- Склейка детекций между тайлами:
  - global NMS по bbox: `GLOBAL_NMS_IOU = 0.50`
- Для каждой маски/полигона сохраняются:
  - bbox `(x1,y1,x2,y2)`
  - центр `(cx,cy)` (по centroid полигона; опционально центр масс по растру `USE_RASTER_COM=False`)
  - confidence
  - сам полигон в пикселях.

### Приведение футпринтов MS Buildings в координаты пикселей
- Футпринты приходят в CRS GeoTIFF (после `to_crs(tif_crs)`).
- Для матчинга каждый футпринт переводится в bbox в пикселях через `inv_aff = ~ds.transform`.
- Для каждого футпринта формируется запись:
  - `fp_idx`, bbox `(x1,y1,x2,y2)` и центр `(cx,cy)`.

### Матчинг футпринтов и YOLO-детекций
Матчинг решает задачу 1-to-1: какой футпринт соответствует какой YOLO-детекции.

**Правила кандидатов:**
- bbox футпринта и bbox детекции должны пересекаться (`bbox_intersects`),
- расстояние между центрами ≤ `MAX_DIST_PX = 120`.

**Оптимизация:**
- строится cost matrix расстояний,
- затем решается assignment через `scipy.optimize.linear_sum_assignment` (Hungarian algorithm),
- пары, которые не получили валидного cost, отбрасываются.

**Итоги матчинга:**
- `matches`: пары `(fp_idx, det_j, dist_px)`
- `unmatched_fp`: футпринты без пары → **ToDelete**
- `unmatched_det`: детекции без пары → **игнорируем** (не добавляем в CVAT)

### Уточнение сдвига футпринтов по IoU против YOLO-масок
Для матченных пар делается уточнение сдвига футпринта, чтобы компенсировать геосдвиг/несовпадение привязки.

**Базовый сдвиг:**
- `dx0 = det_cx - fp_cx`, `dy0 = det_cy - fp_cy`

**Уточнение:**
- берётся ROI вокруг пары (bbox обеих геометрий) с паддингом `IOU_ROI_PAD = 40`,
- внутри ROI перебирается сетка сдвигов вокруг `(dx0,dy0)`:
  - радиус `IOU_RADIUS_PX = 18` (обычно 12..30),
- на каждом сдвиге строятся бинарные маски:
  - футпринт (сдвинутый) vs YOLO-полигон,
  - считается IoU (`iou_masks`),
- выбирается `(dx,dy)` с максимальным IoU.
- YOLO-маска может дилатироваться на `DILATE_DET_PX = 1` (0..2), чтобы уменьшить чувствительность к “тонким” разницам.

**Результат:**
- матченные футпринты сдвигаются в world-координатах (`pixel_delta_to_world_delta` + `shapely.translate`),
- сохраняется отчёт `*_shifts_report.csv` (dx0, dx_best, best_iou, conf и т.п.).

### Разделение на два класса для CVAT
- **Buildings**: матченные футпринты после уточнения сдвига (`gdf_buildings = gdf_shifted[keep_mask]`)
- **ToDelete**: футпринты без пары (из исходного `gdf_raw`)

### Экспорт в CVAT (XML, 2 лейбла)
- Лейблы фиксированы:
  - `LABEL_BUILDINGS = "Buildings"`
  - `LABEL_TODELETE  = "ToDelete"`
- Геометрия экспортируется как полигоны в пикселях:
  - упрощение: `SIMPLIFY_PX = 1.6` (preserve_topology=True),
  - отсев маленьких: `MIN_AREA_PX2 = 20.0`,
  - координаты clamp в [0..W-1], [0..H-1].
- Выход: `*_cvat_2labels.xml`  
  (одна картинка `image id=0`, все полигоны — `source="auto"`)

### Визуальные артефакты для разметчика
1) **PNG с сеткой и нумерацией** для удобной навигации по сцене:
   - шаг сетки: `GRID_STEP_PX = 500`
   - режим подписей: `GRID_LABEL_MODE = "border"` *(в коде реализована функция нумерации клеток; режим “border” заложен как настройка на будущее)*
2) **PNG-отчёт (overlay)**:
   - **Buildings** — зелёный контур
   - **ToDelete** — красный контур
   - лимит отрисовки: `MAX_DRAW = 6000`
   - толщина: `LW_SCALE = 0.5`

### Выходные файлы и упаковка
Для каждого GeoTIFF создаются два zip-архива:

1) Основной архив для разметчика:  
   `OUT_ROOT/<scene>.zip` содержит:
   - `<scene>_grid.png`
   - `<scene>_cvat_2labels.xml`
   - `<scene>_report_buildings_todelete.png`

2) Архив с “служебными” материалами:  
   `OUT_ROOT/<scene>_extras.zip` содержит:
   - `<scene>_raw.geojson` (сырые футпринты)
   - `<scene>_Buildings_shifted.geojson` (Buildings после сдвига)
   - `<scene>_ToDelete.geojson` (не матченные футпринты)
   - `<scene>_shifts_report.csv` (таблица сдвигов/IoU)

После упаковки временная папка сцены удаляется (`shutil.rmtree(work_dir)`).

### Запуск и итоговый отчёт по всем сценам
- Запуск: `run_all()` обходит все `*.tif/*.tiff` в `INPUT_DIR`.
- По каждой сцене собираются метрики:
  - `n_footprints_raw`, `n_buildings`, `n_todelete`
  - `n_detections`, `n_matches`
  - `n_unmatched_fp`, `n_unmatched_det`
- По завершении формируется общий Excel-репорт:  
  `summary_buildings_<timestamp>.xlsx` с листами:
  - `summary` (по файлам)
  - `totals` (суммы по датасету)

### Итог
Этот ноутбук строит **автопредразметку** “сырых” MS Buildings футпринтов, привязанную к пиксельной сетке GeoTIFF и улучшенную через **YOLO-seg + IoU-based shift refinement**.  
В результате получаем удобный комплект для CVAT:
- готовый XML с 2 лейблами (**Buildings** / **ToDelete**),
- сетку для навигации,
- наглядный overlay-отчёт,
- плюс служебные GeoJSON/CSV для аудита качества и отладки.

Последующая разметка выполнялась совместно несколькими участниками (Stepik ID: 597996051, 644912393, 210911504, 1124051397). Снимки были распределены пропорционально вкладу участника.

